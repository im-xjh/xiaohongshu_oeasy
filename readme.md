# 项目说明文档

本项目已上传至：

https://github.com/im-xjh/xiaohongshu_oeasy

视频解说发布：https://www.bilibili.com/video/BV1XrfUYLErV/

本项目旨在从小红书爬取指定关键词的笔记数据，随后对文本内容进行清洗、分词、停用词过滤、关键词提取、主题模型（LDA）分析，并可视化生成词云图、语义网络共现图，以及以网页的形式呈现数据分析结果。整个流程主要包含以下几个阶段：

---

## 一、数据爬取与存储

### `0-mian.py`

这个脚本的主要目标是通过 **Selenium** 驱动 **Chrome** 浏览器，从小红书网站上按照关键词进行检索，滚动加载笔记并获取每篇笔记的详情数据。脚本逻辑如下：

1. **浏览器配置**：指定了 `chromedriver` 的路径，通过 `Options` 自定义了浏览器的部分参数（如远程调试端口、禁用沙箱、使用用户数据目录以保留登录状态等）。
2. **用户登录**：脚本先打开小红书主页，提示用户手动登录，一旦登录成功按回车继续。
3. **关键词搜索**：脚本逐个读取预先定义好的关键词列表,对每个关键词编码后拼接到搜索链接打开。
4. **滚动获取笔记链接**：通过 `scroll_and_get_links` 函数，不断向下滚动页面，利用 `BeautifulSoup` 解析已加载的部分并提取笔记链接，在达到指定数量或无法继续加载时停止。
5. **详情抓取**：对每条笔记链接调用 `extract_note_details` 函数进入笔记详情页，解析包括：
    - 笔记图片
    - 用户主页、头像、昵称
    - 标题、标签、文本内容
    - 时间、点赞数、收藏数、评论数
6. **数据输出**：爬取到的所有笔记详情，一方面以追加行的方式写入到 `jsonl_output_file`（`notes.jsonl`），一方面也保存到 CSV 文件。最后脚本打印整体抓取结果并退出浏览器。

---

## 二、初步清洗与数据处理

### `1.preprocess.py`

该脚本对爬取好的原始笔记数据进行初步清洗和结构化处理，主要功能如下：

1. **读取 JSONL 文件**：定义 `read_jsonl` 函数，通过一行行的方式读取并解析 JSON。
2. **提取笔记 ID**：`extract_id` 函数通过正则匹配从笔记链接里获取唯一 ID，方便后续去重或标识。
3. **数值处理**：`process_numbers` 函数将点赞、收藏、评论等字符串（如“1.2万”）统一转换为整型数值。
4. **时间处理**：`process_time` 函数基于小红书常见时间格式做转换，如“今天 xx:xx”、“昨天 xx:xx”或“x 天前”等，都映射到实际日期或保留可读信息。
5. **筛掉定位在中国省份的 IP**：若笔记的时间后额外带了 IP 属地且位于 `CHINA_REGIONS` 列表里，则跳过不纳入结果。
6. **去重**：根据笔记 ID 建立字典，确保同一笔记不被重复记录。
7. **重排字段顺序**：`restructure_data` 函数按指定 `keys_order` 排列输出，以便后续进行统一的 JSONL 存储。
8. **写出处理结果**：将处理后的记录写到新的 JSONL。

---

## 三、文本分词与停用词过滤

### `2-tokenization.py`

这里开始对文本内容本身进行 **分词、去停用词** 等预处理，以便后续做 TF-IDF、主题模型和可视化分析。主要流程：

1. **停用词加载**：在 `load_stopwords` 函数中，分别从中文停用词文件、英文停用词文件以及 NLTK 的英文停用词库合并得到一个总的停用词集合。
2. **中英混合分割**：`split_text_to_cn_en` 函数用正则把文本拆成中文片段和非中文片段。
3. **分词**：
    - 中文片段：使用 `jieba.cut` 分词
    - 英文片段：用 `nltk.word_tokenize` 分词并转小写
4. **停用词过滤**：对分词结果进行去重、去除无效字符和停用词。
5. **整体处理**：对原始 JSONL 数据中每条记录的 `text` 字段进行上述操作，并在 DataFrame 中新增 `text_processed` 列。
6. **输出**：分词完成后将结果写回到 `preprocessed_data.jsonl`。

---

## 四、TF-IDF 关键词提取与词云可视化

### `3-wordcloud.py`

在该脚本中，我们先对分词结果进行 **TF-IDF** 计算，从而找出文本中的高权重词，再将结果可视化为词云。主要流程：

1. **读取分词后数据**：从 `preprocessed_data.jsonl` 获取所有笔记的 `text_processed`。
2. **停用词**：可再次加载外部停用词以确保一致性。
3. **TF-IDF**：通过 `TfidfVectorizer`（来自 `sklearn.feature_extraction.text`）对文本集合进行向量化。参数如 `max_features=100` 用于取最高权重的 100 个词。
4. **权重聚合**：将生成的 TF-IDF 矩阵按列求和，得到每个词在整份语料中的总权重，按照从高到低排序输出。
5. **保存结果**：把词和对应分数保存成 JSON（例如 `tfidf_result.json`）。
6. **词云生成**：加载这些词和得分，借助 `WordCloud` 来生成词云图并保存成 `wordcloud.png`。这里使用了中文字体 `SourceHanSansCN-Regular.otf` 来确保中文正常显示。

---

## 五、主题模型（LDA）与可视化

### `4-LDA.py`

当我们想要从大量笔记中挖掘隐藏主题时，Latent Dirichlet Allocation (LDA) 便是一种常用算法。脚本实现如下功能：

1. **加载预处理数据**：从 `preprocessed_data.jsonl` 中读取所有 `text_processed`。
2. **CountVectorizer**：与前面 TF-IDF 类似，这里使用 `CountVectorizer` 将文本转换成词频矩阵，设置了一定的 `max_df`, `min_df` 等参数做筛选。
3. **主题范围**：定义了 `min_topics=4`，`max_topics=12`，会循环尝试从 4 个主题到 12 个主题的各种模型，每次训练完成后计算困惑度（Perplexity）和一致性（Coherence）。
4. **模型训练**：通过 `LatentDirichletAllocation` 训练出相应数目的主题分布，并把结果可视化输出到 HTML（使用 `pyLDAvis` 库）。
5. **结果保存**：将每篇文档的主题分布、主要主题 ID 等信息写回 JSONL 文件；同时把每个主题的高频词汇写到 CSV。
6. **评估指标曲线**：最后绘制了随主题数变化的 **困惑度** 曲线和 **一致性** 曲线，并保存为图片。根据指标曲线，采用4个主题的聚类结果。

---

## 六、关键词共现网络构建

### `5-network.py`

采用 **词共现网络** 来观察关键词之间的关联度。此脚本的实现逻辑如下：

1. **读取 JSONL**：同样从分词结果的 JSONL 中读取数据。
2. **共现网络**：定义 `build_co_occurrence_network`，在给定的 `window_size` 内，若两个词出现在同一窗口，则在图中二者之间建立加权边，权重代表共现次数。
3. **全局网络合并**：对所有文本的共现图做合并，得到一个全局 **NetworkX** 图对象。
4. **生成节点与边表**：将 `G.nodes` 输出成 CSV，将 `G.edges` 输出成另一份 CSV，用于在 Gephi 可视化工具中直接导入。

---

## 七、图片下载与合成拼接

### 7.1 `6-imagedownload.py`

在爬取笔记时已将图片 URL 一并存储，于是批量下载全部笔记图片：

1. **读取 JSONL**：获取笔记里的 `images` 列表。
2. **逐张下载**：遍历 `images`，用 `requests` GET 获取图片二进制流。若成功响应，则保存到本地文件。
3. **命名策略**：用笔记的 `title` 加编号组成文件名，保证部分区分。
4. **输出提示**：对于下载失败则打印报错。

### 7.2 `7-imagecollage.py`

将所有下载好的小图片拼成一张大的拼贴图：

1. **指定网格大小**：如 `grid_rows=20`，`grid_cols=50`，则最终图像会拼接成 20 行 × 50 列。
2. **计算单张缩放尺寸**：先读取一张图片，结合最大宽度 `max_width=5000` 算出缩放后单张宽度与高度，避免图像文件过大。
3. **顺序拼接**：先从随机打乱的图片列表中取图，逐行拼接，再把整行贴到最终画布上。
4. **输出**：保存成 `final_collage.jpg`。用于做视觉展示。